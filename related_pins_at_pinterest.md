# Related Pins at Pinterest: The Evolution of a Real-World Recommender System

[//]: # (Image References)

[image1]: ./img/related_pins_sys.png
[image2]: ./img/related_pins_pin2vec.png
[image3]: ./img/related_pins_actions.png
[image4]: ./img/related_pins_memboost_score.png
[image5]: ./img/related_pins_feedback.png

Source: [Pinterest Labs](https://labs.pinterest.com/user/themes/pinlabs/assets/paper/p2p-www17.pdf)

Related Pins leverages human-curated content to provide personalized recommendations of pins based on a given query pin.

**Key metric:** `Related Pins Save Propensity`, which is defined as the number of users who have saved a Related Pins recommended pin divided by the number of users who have seen a Related Pins recommended pin.

## Related Pins System Overview

The Related Pins system comprises three major components. The components were introduced to the system over time, and they have each evolved dramatically in their own right.

![alt text][image1]

**Candidate Generation.** We first narrow the candidate set - the set of pins eligible for Related Pins recommendations - from billions to roughly 1,000 pins that are likely related to the query pin.

**Memboost.** A portion of our system memorizes past engagement on specific query and result pairs.

**Ranking.** A machine-learned ranking model is applied to the pins, ordering them to maximize our target engagement metric.

## Evolution of Candidates

### Board co-occurrence

#### Heuristic candidates.

The original Related Pins were computed in an offline Hadoop Map/Reduce job: we mapped over the set of boards and output pairs of pins that occured on the same board.

There are too many pairs of possible pins, so `pairs are randomly sampled` to produce approximately the same number of candidates per query pin.

We further added a `heuristic relevance score`, based on rough text and category matching. The score was hand-tuned by inspecting example results.

#### Online Random Walk

Shortcomings of previous approaches:
 * heuristic score did not attempt to maximize board co-occurence.
 * rare pins did not have many candidates.
 
To address these limitations we moved to generating candidates at serving time through an online traversal of the board-to-pin graph.

### Session Co-occurence

Shortcomings of previous approaches:
 * boards are often too broad, so any given pair of pins on a board may only be tangentially related.
 * boards may also be too narrow: for example, a whiskey and a cocktail made with that whiskey might be pinned in close succesion to different boards.
 
Both these shortcomings can be addressed by incorporating the temporal dimension of user behaviour: pins saved during the same session are typically related in some way.

We built Pin2Vec to harness these session co-occurence signals. Pin2Vec is a learned embedding of the N most popular in a d-dimensional space, with the goal of minimizing the distance between pins which are saved in the same session:
 * each training example is a pair of pins that are saved by the same user within a certain time window.
 * given one of the pins as input, an embedding matrix maps pin IDs to vectors, and a softmax layer is used to map the embeddings back into a predicted output pin ID.
 * the other pin in the pair is given as the expected output, and we train the embedding by minimizing the cross-entropy loss.
 * negative examples are sampled to make the optimization tractable.

At serving time, when the user queries one of the N pins, we generate candidate pins by looking up its nearest neighbours in the embedding space.
 
![alt text][image2]

### Supplemental Candidates

**Search-based candidates.** We generate candidates by leveraging Pinterest's text-based search, using the query pin's annotations (words from the web link or description) as query tokens. These search-based candidates tend to be less specifically relevant than those generated from board co-occurrence, but offer a nice trade-ff from an exploration perspective.

**Visually similar candidates.** 
 * If the query image is a near-duplicate, then we add the Related Pins recommendations for the duplicate image to the result.
 * If no near-duplicate is identified, then we use the Visual Search backend to return visually similar images, based on a nearest-neighbour lookup of the query's visual enbedding vector.

### Segmented Candidates

**Problem:** local content was not generated by candidate sources.

To solve this, we generated `additional candidate sets segmented by locale` for many of the above generation techniques.

## Evolution of Memboost

**Motivation:** Initial version of Related Pins already recieved a high amount of engagement. As a first step toward learning from our massive engagement logs, we built Memboost `to memorize the best pins for each query`.

We initially wanted to simply incorporate the historical click-through rate of each result.

**Problem:** Log data is subject to a strong `position bias`: items shown in earlier positions are more likely to be clicked on.

To account for this, we instead chose to compute `clicks over expected clicks`. Let:
 * clicks(q, r) be the total number of clicks received by result pin r on query pin q.
 * i<sub>p, k</sub>(q, r) be the number of impressions result pin r on query pin q received on platform p and rank k.
 * each impression contributes a certain  fractional number of `expected clicks`, based on the global prior clickrate<sub>p, k</sub> for that rank and platform: Eclicks(q, r) = (sum over all p and k) i<sub>p, k</sub>(q, r) * clickrate<sub>p, k</sub>.

We extended these definitions to other engagement actions:

![alt text][image3]

The overall `Memboost score` is:

![alt text][image4]

 * to get a zero-centered score where positive and negative values would indicate that the result was engaged with more and less than expected, respectively, we use the logarithm.
 * we apply additive smoothing to handle items with low action / impression counts.

The Memboost scores are added with a coefficient to adjust the existing scores of pins.

Until recently, the Memboost weights were hand-tuned. Now we moved to `Memboost as a feature`, where the intermediate Memboost values (clicks, Eclicks, ...) are fed as features into the machine-learned ranker.

### Memboost Insertion

Memboost insertion re-inserts the top n result with the highest aggregate Memboost score if they are not already in the incoming result set.

### Discussion

Memboost as a whole introduces significant system complexity by adding feedback loops in the system.

## Evolution of Ranking

In our application, the ranker re-orders candidate pins in the context of a particular query Q, which comprises the query pin, the viewing user, and user context.

**Version 1: Memboost training data, relevance pair labels, pairwise loss, and linear RankSVM model.**

* **Supervision signal:** Memboost scores.
* **Loss function:** pairwise ranking loss.
* **Training data:** we explicitly sampled pairs of pins (r<sub>1</sub>, r<sub>n</sub>), (r<sub>n</sub>, r<sub>rand</sub>) for each query, where r<sub>1</sub>, r<sub>n</sub> are the results with highest and lowest Memboost scores, respectively, for a given query. r<sub>rand</sub> is a randomly generated popular pin from Pinterest.
* **Model:** linear RankSVM.

**Version 2: Moved to individual Related Pins sessions.**

Shortcomings of previous approaches:
 * using Memboost data inherently `precludes personalization` (because it is aggregated over many users).
 * only popular content had enough interaction for reliable Memboost data.
 
These limitations motivated the folowing changes:
 * **Supervision signal:** user interactions with Related Pins (save > long_click > click > closeup > impression_only).
 * **Training data:** we trim the logged set of pins, taking each engaged pin as well as two pins immediately preceding it in rank order (under the assumption that the user probably saw those pins).
 
**Version 3: Moved to RankNet GBDT Model.**

Shortcomings of previous approaches:
 * linear model can not capture complex dependencies.
 
To avoid these downsides, we moved to gradient-boosted decision trees:
 * **Model:** GBDT RankNet.
 
**Version 4: Moved to pointwise classification loss, binary labels, and logistic GBDN model.**

 * **Supervision signal:** since our primary target metric in online experiments is the prospensity of users to save result pins, using training examples which also include closeups and clicks seemed counterproductive since these actions may not reflect save prospensity. We found that giving examples simple binary labels ("saved" and "not saved") and reweighting positive examples to combat class imbalance proved effective at increasing save prospensity.
 * **Loss function:** pointwise logistic loss.

 ### Previous-Model Bias

**Problem:** Because engagement logs are used for training, we introduced a `direct feedback loop`: the model that is currently deployed dramatically impacts the training examples produced for future models.

To alleviate this "previous-model" bias in the training data, we allocate a small percentage of traffic for "unbiased data collection": for these requests, we show a random sample from all our candidate sources, randomly ordered without ranking.

To avoid degrading any particular user's experience too much, each user is served unranked pins on only a small random subset of queries.

![alt text][image5]
