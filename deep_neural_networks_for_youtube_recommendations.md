# Deep Neural Networks for YouTube Recommendations

Source: [paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf)

[//]: # (Image References)

[image1]: ./img/youtube_deep_overview.png
[image2]: ./img/youtube_deep_softmax.png

## System Overview

The system is comprised of two neural networks: one for `candidate generation` and one for `ranking`.

![alt text][image1]

The candidate generation network takes events from the userâ€™s YouTube activity history as input and retrieves a small subset (hundreds) of videos from a large corpus.

The ranking network assigns a score to each video according to a desired objective function using a rich set of features describing the video and user.

The two-stage approach to recommendation allows us to make recommendations from a very large corpus (millions) of videos while still being certain that the small number of videos appearing on the device are personalized and engaging for the user. Furthermore, this design enables blending candidates generated by other sources,

## Candidate Generation

During candidate generation, the enormous YouTube corpus is winnowed down to hundreds of videos that may be relevant to the user.

### Recommendation as Classification

We pose recommendation as extreme multiclass classification where the prediction problem becomes accurately classifying a specific video watch at time t among millions of videos (classes) from a corpus based on a user and context.

![alt text][image2]

Although explicit feedback mechanisms exist on YouTube we use the implicit feedback of watches to train the model, where a user completing a video is a positive example.

To efficiently train such a model with millions of classes, we rely on a technique to sample negative classes from the background distribution and then correct for this sampling via importance weighting. For each example the cross-entropy loss is minimized for the true label and the sampled negative classes. In practice several thousand negatives are sampled.

At serving time we need to compute the most likely N classes (videos) in order to choose the top N to present to the user. Scoring millions of items under a strict serving latency of tens of milliseconds requires an approximate scoring scheme sublinear in the number of classes. Since calibrated likelihoods from the softmax output layer are not needed at serving time, the scoring problem reduces to a nearest neighbor search in the dot product space for which general purpose libraries can be used.
